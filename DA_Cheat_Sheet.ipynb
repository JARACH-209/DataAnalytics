{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DA_Cheat_Sheet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjZk9VRoDtfE1vwsRmLJne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JARACH-209/DataAnalytics/blob/master/DA_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpWVMWvZoXh1"
      },
      "source": [
        "#Data Analysis Code Snippet Cheat-Sheet\n",
        "\n",
        "### **Author : Achal Dixit**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53V5-WWkpAuH"
      },
      "source": [
        "General Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFp71SGpAYr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import r2_score\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In6MjUvfq9MK"
      },
      "source": [
        "EDA Techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TgMVZyrAA1"
      },
      "source": [
        "#To find the null, means, etc.\n",
        "#.desribe() and .info() can also be used\n",
        "def ames_eda(df): \n",
        "    eda_df = {}\n",
        "    eda_df['null_sum'] = df.isnull().sum()\n",
        "    eda_df['null_pct'] = df.isnull().mean()\n",
        "    eda_df['dtypes'] = df.dtypes\n",
        "    eda_df['count'] = df.count()\n",
        "    eda_df['mean'] = df.mean()\n",
        "    eda_df['median'] = df.median()\n",
        "    eda_df['min'] = df.min()\n",
        "    eda_df['max'] = df.max()\n",
        "    \n",
        "    return pd.DataFrame(eda_df)ames_eda(train)\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "# Plot to visualize the dataset using seaborn for classification points\n",
        "\n",
        "sns.pairplot(df_cancer, hue = 'target', vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqS3U1sb3i8n"
      },
      "source": [
        "To visualize missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1nwxAOO3iMI"
      },
      "source": [
        "heat_map = sns.heatmap(df.isnull(), yticklabels = False, cbar = True, cmap = \"PuRd\", vmin = 0, vmax = 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2aeK5_voXAf"
      },
      "source": [
        "To find Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCO7fsTdoOR1"
      },
      "source": [
        "kendall = abs(DATAFRAME.corr(method=\"kendall\").Price).to_dict()\n",
        "pearson = abs(DATAFRAME.corr(method=\"pearson\").Price).to_dict()\n",
        "spearman = abs(DATAFRAME.corr(method=\"spearman\").Price).to_dict()\n",
        "\n",
        "print(\"\\t\\tPearson\\t\\tKendall\\t\\tSpearman\")\n",
        "for p,k,s in zip(pearson.items(),kendall.items(),spearman.items()):\n",
        "    if p[1] >.50 or s[1] > .50 or k[1] > .50:\n",
        "        print(\"\\t{}\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}\".format(p[0],p[1],k[1],s[1]))\n",
        "\n",
        "\n",
        "temp = abs(boston_df.corr('pearson').Price).to_dict()\n",
        "from collections import Counter\n",
        "var = Counter(temp)\n",
        "top_features = var.most_common(4)\n",
        "del top_features[0]\n",
        "print(top_features)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "#Correlation Heatmaps\n",
        "'''1'''\n",
        "correlations = train.corrwith(train['saleprice']).iloc[:-1].to_frame()\n",
        "correlations['abs'] = correlations[0].abs()\n",
        "sorted_correlations = correlations.sort_values('abs', ascending=False)[0]\n",
        "fig, ax = plt.subplots(figsize=(10,20))\n",
        "sns.heatmap(sorted_correlations.to_frame(), cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);\n",
        "\n",
        "'''2'''\n",
        "def correlation_heatmap(train):\n",
        "    correlations = train.corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
        "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
        "    plt.show();\n",
        "    \n",
        "correlation_heatmap(train)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "color_dic = {0:'red', 1:'blue'} \n",
        "target_list = list(data['target'])\n",
        "colors = list(map(lambda x: color_dic.get(x), target_list))#Plotting the scatter matrix\n",
        "sm = pd.plotting.scatter_matrix(data1[features_mean], c= colors, alpha=0.4, figsize=((10,10)))\n",
        "plt.suptitle(\"How well a feature separates the Malignant Points from the Benign Ones\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvWhVR3jpFD5"
      },
      "source": [
        "Scatter Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wa2cNr3pH6B"
      },
      "source": [
        "#Normal scatter for quick lookup\n",
        "fig,ax = plt.subplots(figsize= (7,5))\n",
        "ax.scatter(boston_df.CRIM,boston_df.Price,s=3)\n",
        "\n",
        "#------------------------------------------------------------\n",
        "fig1,ax1 = plt.subplots()\n",
        "fig1.set_size_inches(12,7.5)\n",
        "ax1.set_ylabel('Price')\n",
        "ax1.set_xlabel('Low status Popolation %')\n",
        "ax1.set_title('Relationship Between Price and Low status Population')\n",
        "c = boston_df['Price']\n",
        "plt.scatter(boston_df.LSTAT, boston_df.Price, c=c, cmap = 'copper_r', alpha =0.6)  \n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Price')\n",
        "\n",
        "''' cmap values = \"copper\" , \"Reds\", \"autumn\", \"PuRd\" add _r to reverse the scaling'''\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "# Includes how to add legends\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(9,5))\n",
        "scatter1 = ax.scatter(perimeter_test,smooth_test,c= y_test,alpha=.7,marker=\"d\",cmap=\"bwr_r\",s=60)\n",
        "scatter2 = ax.scatter(perimeter_test,smooth_test,c= y_pred,marker=\".\",s=25,cmap=\"gnuplot\")\n",
        "ax.grid(False)\n",
        "\n",
        "# produce a legend with the unique colors from the scatter\n",
        "legend1 = ax.legend(*scatter1.legend_elements(),\n",
        "                    loc=\"upper right\", title=\"Predicted Classes\")\n",
        "ax.add_artist(legend1)\n",
        "\n",
        "legend2 = ax.legend(*scatter2.legend_elements(),\n",
        "                    loc=\"lower right\", title=\"Actual Classes\")\n",
        "ax.add_artist(legend2)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "\n",
        "''' Plots decision boundaries for diffrent calssifiers'''\n",
        "\n",
        "def plot_decision_boundaries(X, y, model_class, **model_params):\n",
        "    try:\n",
        "        X = np.array(X)\n",
        "        y = np.array(y).flatten()\n",
        "    except:\n",
        "        print(\"Coercing input data to NumPy arrays failed\")\n",
        "    # Reduces to the first two columns of data\n",
        "    reduced_data = X[:, :2]\n",
        "    # Instantiate the model object\n",
        "    model = model_class(**model_params)\n",
        "    # Fits the model with the reduced data\n",
        "    model.fit(reduced_data, y)\n",
        "\n",
        "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
        "    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
        "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
        "    # Meshgrid creation\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Obtain labels for each point in mesh using the model.\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    # Predictions to obtain the classification results\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12,7))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
        "    plt.xlabel(\"Feature-1\",fontsize=5)\n",
        "    plt.ylabel(\"Feature-2\",fontsize=5)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "    return plt\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Decision boundary linear\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "   # setup marker generator and color map\n",
        "   markers = ('s', 'x', 'o', '^', 'v')\n",
        "   colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "   cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "   # plot the decision surface\n",
        "   x1_min, x1_max = X[:,  0].min() - 1, X[:, 0].max() + 1\n",
        "   x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "   xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "   np.arange(x2_min, x2_max, resolution))\n",
        "   Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "   Z = Z.reshape(xx1.shape)\n",
        "   plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "   plt.xlim(xx1.min(), xx1.max())\n",
        "   plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "   # plot class samples\n",
        "   for idx, cl in enumerate(np.unique(y)):\n",
        "      plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "      alpha=0.8, c=cmap(idx),\n",
        "      marker=markers[idx], label=cl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYpmWkZp1RJ"
      },
      "source": [
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf1hotRbp3LW"
      },
      "source": [
        "#Train and Test set split into 80% and 20% respectively\n",
        "x_train, x_rest, y_train, y_rest = train_test_split(x,y,test_size = .4,random_state =0)\n",
        "\n",
        "#Rest of the 40% set split into equal parts of Train and Validation set \n",
        "x_test, x_val, y_test, y_val = train_test_split(x_train,y_train,test_size = .5,random_state = 0)\n",
        "\n",
        "#Therefore : Train = 60%, Test = 20% and Validation = 20%\n",
        "print(len(x_train),len(y_train))\n",
        "print(len(x_val),len(y_val))\n",
        "print(len(x_test),len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nWqCdWp9LJ"
      },
      "source": [
        "Scaling and Reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYTY7L8OqCex"
      },
      "source": [
        "scaler = preprocessing.StandardScaler().fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "y_train = y_train.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "y_val = y_val.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJQAxltJPOWW"
      },
      "source": [
        "Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35ZtgYgPIuW"
      },
      "source": [
        "def grid_init():\n",
        "    grid = list()\n",
        "    for i in range(-8,1):\n",
        "        grid += [no*(10**i) for no in range(1, 10)]\n",
        "    return grid\n",
        "\n",
        "def grid_search():\n",
        "    best_lrate = None\n",
        "    accuracy = None\n",
        "    best_classifier = None\n",
        "    lrate_acc = {}\n",
        "    best_acc = 1e5 \n",
        "    best_iter = 100\n",
        "\n",
        "    iterations = [100,500,1000,1500,2000,5000,7500,10000,12500,15000,20000,35000,50000,100000,1000000,3000000]\n",
        "    grid = grid_init()\n",
        "    for iter in iterations:\n",
        "        for lrate in grid:\n",
        "            clf = SGDRegressor(max_iter=iter,learning_rate=\"constant\",eta0=lrate)\n",
        "            clf.fit(x_train,y_train)\n",
        "            accuracy = mean_absolute_error(y_test,clf.predict(x_test))\n",
        "            lrate_acc[str(accuracy)] = accuracy\n",
        "\n",
        "            if accuracy < best_acc:\n",
        "                best_acc = accuracy\n",
        "                best_lrate = lrate\n",
        "                best_classifier = clf\n",
        "                best_iter = iter\n",
        "    print(best_acc)\n",
        "    return best_classifier, best_lrate, lrate_acc , iter\n",
        "\n",
        "\n",
        "clf_sdg, lrate, lrate_accuracy, iterations = grid_search()\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "pen = ['l1','l2','elasticnet']\n",
        "learning_rate = [\"invscaling\",\"constant\",\"optimal\"]\n",
        "eta = grid_init()\n",
        "\n",
        "param_grid = dict(penalty = pen,eta0=eta,learning_rate = learning_rate)\n",
        "\n",
        "sdg = SGDRegressor(max_iter = 3000000)\n",
        "\n",
        "gridcv = GridSearchCV(estimator=sdg,param_grid = param_grid,scoring='neg_mean_absolute_error') \n",
        "\n",
        "grid_clf = gridcv.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErvjRZpqhB4"
      },
      "source": [
        "Evaluation and Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSyKOeuqf_9"
      },
      "source": [
        "clf_sdg = SGDRegressor(max_iter= 50000,eta0=0.0001,learning_rate='constant')\n",
        "#learning rate is default = 0.01 as constant = eta0\n",
        "\n",
        "clf_sdg.fit(x_train,y_train)\n",
        "\n",
        "y_hat_test = clf_sdg.predict(x_test)\n",
        "y_hat_val = clf_sdg.predict(x_val)\n",
        "y_hat_train = clf_sdg.predict(x_train)\n",
        "\n",
        "test_score = clf_sdg.score(x_test,y_test)\n",
        "train_score = clf_sdg.score(x_train,y_train)\n",
        "val_score = clf_sdg.score(x_val,y_val)\n",
        "\n",
        "print(\"Train R2-score : \\t\\t%.2f\" % r2_score(train_y_hat , y_train) )\n",
        "print(\"Test R2-score : \\t\\t%.2f\" % r2_score(test_y_hat , y_test) )\n",
        "print(\"Validation R2-score : \\t\\t%.2f\" % r2_score(val_y_hat , y_val) )\n",
        "print(\"------------------------------------\")\n",
        "print(\"Train score : \\t\\t\\t%.2f\"%train_score)\n",
        "print(\"Test score : \\t\\t\\t%.2f\"%test_score)\n",
        "print(\"Validation Score : \\t\\t%.2f\"%val_score)\n",
        "\n",
        "print(\"RMSE test : %.4f\"%math.sqrt(mean_squared_error(y_test,test_y_hat)))\n",
        "print(\"RMSE train : %.4f\"%math.sqrt(mean_squared_error(y_train,train_y_hat)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}