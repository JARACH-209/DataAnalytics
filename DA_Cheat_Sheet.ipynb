{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DA_Cheat_Sheet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFkPA5HM5C25ToIhbmBXHn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JARACH-209/DataAnalytics/blob/master/DA_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpWVMWvZoXh1",
        "colab_type": "text"
      },
      "source": [
        "#Data Analysis Code Snippet Cheat-Sheet\n",
        "\n",
        "### **Author : Achal Dixit**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53V5-WWkpAuH",
        "colab_type": "text"
      },
      "source": [
        "General Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFp71SGpAYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import r2_score\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In6MjUvfq9MK",
        "colab_type": "text"
      },
      "source": [
        "EDA Techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TgMVZyrAA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To find the null, means, etc.\n",
        "#.desribe() and .info() can also be used\n",
        "def ames_eda(df): \n",
        "    eda_df = {}\n",
        "    eda_df['null_sum'] = df.isnull().sum()\n",
        "    eda_df['null_pct'] = df.isnull().mean()\n",
        "    eda_df['dtypes'] = df.dtypes\n",
        "    eda_df['count'] = df.count()\n",
        "    eda_df['mean'] = df.mean()\n",
        "    eda_df['median'] = df.median()\n",
        "    eda_df['min'] = df.min()\n",
        "    eda_df['max'] = df.max()\n",
        "    \n",
        "    return pd.DataFrame(eda_df)ames_eda(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2aeK5_voXAf",
        "colab_type": "text"
      },
      "source": [
        "To find Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCO7fsTdoOR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kendall = abs(DATAFRAME.corr(method=\"kendall\").Price).to_dict()\n",
        "pearson = abs(DATAFRAME.corr(method=\"pearson\").Price).to_dict()\n",
        "spearman = abs(DATAFRAME.corr(method=\"spearman\").Price).to_dict()\n",
        "\n",
        "print(\"\\t\\tPearson\\t\\tKendall\\t\\tSpearman\")\n",
        "for p,k,s in zip(pearson.items(),kendall.items(),spearman.items()):\n",
        "    if p[1] >.50 or s[1] > .50 or k[1] > .50:\n",
        "        print(\"\\t{}\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}\".format(p[0],p[1],k[1],s[1]))\n",
        "\n",
        "\n",
        "temp = abs(boston_df.corr('pearson').Price).to_dict()\n",
        "from collections import Counter\n",
        "var = Counter(temp)\n",
        "top_features = var.most_common(4)\n",
        "del top_features[0]\n",
        "print(top_features)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "#Correlation Heatmap\n",
        "\n",
        "correlations = train.corrwith(train['saleprice']).iloc[:-1].to_frame()\n",
        "correlations['abs'] = correlations[0].abs()\n",
        "sorted_correlations = correlations.sort_values('abs', ascending=False)[0]fig, ax = plt.subplots(figsize=(10,20))\n",
        "sns.heatmap(sorted_correlations.to_frame(), cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvWhVR3jpFD5",
        "colab_type": "text"
      },
      "source": [
        "Scatter Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wa2cNr3pH6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normal scatter for quick lookup\n",
        "fig,ax = plt.subplots(figsize= (7,5))\n",
        "ax.scatter(boston_df.CRIM,boston_df.Price,s=3)\n",
        "\n",
        "#------------------------------------------------------------\n",
        "fig1,ax1 = plt.subplots()\n",
        "fig1.set_size_inches(12,7.5)\n",
        "ax1.set_ylabel('Price')\n",
        "ax1.set_xlabel('Low status Popolation %')\n",
        "ax1.set_title('Relationship Between Price and Low status Population')\n",
        "c = boston_df['Price']\n",
        "plt.scatter(boston_df.LSTAT, boston_df.Price, c=c, cmap = 'copper_r', alpha =0.6)  \n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Price')\n",
        "\n",
        "''' cmap values = \"copper\" , \"Reds\", \"autumn\", \"PuRd\" add _r to reverse the scaling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYpmWkZp1RJ",
        "colab_type": "text"
      },
      "source": [
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf1hotRbp3LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and Test set split into 80% and 20% respectively\n",
        "x_train, x_rest, y_train, y_rest = train_test_split(x,y,test_size = .4,random_state =0)\n",
        "\n",
        "#Rest of the 40% set split into equal parts of Train and Validation set \n",
        "x_test, x_val, y_test, y_val = train_test_split(x_train,y_train,test_size = .5,random_state = 0)\n",
        "\n",
        "#Therefore : Train = 60%, Test = 20% and Validation = 20%\n",
        "print(len(x_train),len(y_train))\n",
        "print(len(x_val),len(y_val))\n",
        "print(len(x_test),len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nWqCdWp9LJ",
        "colab_type": "text"
      },
      "source": [
        "Scaling and Reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYTY7L8OqCex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = preprocessing.StandardScaler().fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "y_train = y_train.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "y_val = y_val.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErvjRZpqhB4",
        "colab_type": "text"
      },
      "source": [
        "Evaluation and Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSyKOeuqf_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_sdg = SGDRegressor(max_iter= 50000,eta0=0.0001,learning_rate='constant')\n",
        "#learning rate is default = 0.01 as constant = eta0\n",
        "\n",
        "clf_sdg.fit(x_train,y_train)\n",
        "\n",
        "y_hat_test = clf_sdg.predict(x_test)\n",
        "y_hat_val = clf_sdg.predict(x_val)\n",
        "y_hat_train = clf_sdg.predict(x_train)\n",
        "\n",
        "test_score = clf_sdg.score(x_test,y_test)\n",
        "train_score = clf_sdg.score(x_train,y_train)\n",
        "val_score = clf_sdg.score(x_val,y_val)\n",
        "\n",
        "print(\"Train R2-score : \\t\\t%.2f\" % r2_score(train_y_hat , y_train) )\n",
        "print(\"Test R2-score : \\t\\t%.2f\" % r2_score(test_y_hat , y_test) )\n",
        "print(\"Validation R2-score : \\t\\t%.2f\" % r2_score(val_y_hat , y_val) )\n",
        "print(\"------------------------------------\")\n",
        "print(\"Train score : \\t\\t\\t%.2f\"%train_score)\n",
        "print(\"Test score : \\t\\t\\t%.2f\"%test_score)\n",
        "print(\"Validation Score : \\t\\t%.2f\"%val_score)\n",
        "\n",
        "print(\"RMSE test : %.4f\"%math.sqrt(mean_squared_error(y_test,test_y_hat)))\n",
        "print(\"RMSE train : %.4f\"%math.sqrt(mean_squared_error(y_train,train_y_hat)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}